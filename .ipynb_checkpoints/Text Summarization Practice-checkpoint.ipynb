{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:39.856138Z",
     "start_time": "2018-02-19T21:05:38.598051Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:39.861630Z",
     "start_time": "2018-02-19T21:05:39.858216Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = 'http://radar.oreilly.com/2010/07/louvre-industrial-age-henry-ford.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:42.708866Z",
     "start_time": "2018-02-19T21:05:39.864742Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:42.845087Z",
     "start_time": "2018-02-19T21:05:42.712509Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "page = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:42.859903Z",
     "start_time": "2018-02-19T21:05:42.848369Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = ' '.join([p.text for p in page.find('div', attrs={'class': 'entry-content'}).find_all('p')])\n",
    "txt = txt.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:42.878306Z",
     "start_time": "2018-02-19T21:05:42.862613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This morning I had the chance to get a tour of The Henry Ford Museum in Dearborn, MI, along with Dal'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:42.913340Z",
     "start_time": "2018-02-19T21:05:42.881422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This morning I had the chance to get a tour of The Henry Ford Museum in Dearborn, MI, along with Dale Dougherty, creator of Make: and Makerfaire, and Marc Greuther, the chief curator of the museum.',\n",
       " 'I had expected a museum dedicated to the auto industry, but it’s so much more than that.',\n",
       " 'As I wrote in my first stunned tweet, “it’s the Louvre of the Industrial Age.”  When we first entered, Marc took us to what he said may be his favorite artifact in the museum, a block of concrete that contains Luther Burbank’s shovel, and Thomas Edison’s signature and footprints.',\n",
       " 'Luther Burbank was, of course, the great agricultural inventor who created such treasures as the nectarine and the Santa Rosa plum.',\n",
       " 'Ford was a farm boy who became an industrialist; Thomas Edison was his friend and mentor.',\n",
       " 'The museum, opened in 1929, was Ford’s personal homage to the transformation of the world that he was so much a part of.',\n",
       " 'This museum chronicles that transformation.',\n",
       " 'The machines are astonishing – steam engines and coal-fired electric generators as big as houses, the first lathes capable of making other precision lathes (the makerbot of the 19th century), a ribbon glass machine that is one of five that in the 1970s made virtually all of the incandescent lightbulbs in the world, combine harvesters, railroad locomotives, cars, airplanes, even motels, gas stations, an early McDonalds’ restaurant and other epiphenomena of the automobile era.',\n",
       " 'Under Marc’s eye, we also saw the transformation of the machines from purely functional objects to things of beauty.',\n",
       " 'We saw the advances in engineering — the materials, the workmanship, the design, over a hundred years of innovation.',\n",
       " 'Visiting The Henry Ford, as they call it, is a truly humbling experience.',\n",
       " 'I would never in a hundred years have thought of making a visit to Detroit just to visit this museum, but knowing what I know now, I will tell you confidently that it is as worth your while as a visit to Paris just to see the Louvre, to Rome for the Vatican Museum, to Florence for the Uffizi Gallery, to St. Petersburg for the Hermitage, or to Berlin for the Pergamon Museum.',\n",
       " 'This is truly one of the world’s great museums, and the world that it chronicles is our own.',\n",
       " 'I am truly humbled that the Museum has partnered with us to hold Makerfaire Detroit on their grounds.',\n",
       " 'If you are anywhere in reach of Detroit this weekend, I heartily recommend that you plan to spend both days there.',\n",
       " 'You can easily spend a day at Makerfaire, and you could easily spend a day at The Henry Ford.',\n",
       " 'P.S.',\n",
       " 'Here are some of my photos from my visit.',\n",
       " '(More to come soon.',\n",
       " 'Can’t upload many as I’m currently on a plane.)',\n",
       " 'Related:']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EOS Detection\n",
    "sentences = nltk.tokenize.sent_tokenize(txt)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:42.934611Z",
     "start_time": "2018-02-19T21:05:42.916266Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'morning',\n",
       " 'I',\n",
       " 'had',\n",
       " 'the',\n",
       " 'chance',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'tour',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Henry',\n",
       " 'Ford',\n",
       " 'Museum',\n",
       " 'in',\n",
       " 'Dearborn',\n",
       " ',',\n",
       " 'MI',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'Dale',\n",
       " 'Dougherty',\n",
       " ',',\n",
       " 'creator',\n",
       " 'of',\n",
       " 'Make',\n",
       " ':',\n",
       " 'and',\n",
       " 'Makerfaire',\n",
       " ',',\n",
       " 'and',\n",
       " 'Marc',\n",
       " 'Greuther',\n",
       " ',',\n",
       " 'the',\n",
       " 'chief',\n",
       " 'curator',\n",
       " 'of',\n",
       " 'the',\n",
       " 'museum',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = [nltk.tokenize.word_tokenize(s) for s in sentences]\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:43.117588Z",
     "start_time": "2018-02-19T21:05:42.937568Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('morning', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('had', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('chance', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('tour', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('Henry', 'NNP'),\n",
       " ('Ford', 'NNP'),\n",
       " ('Museum', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('Dearborn', 'NNP'),\n",
       " (',', ','),\n",
       " ('MI', 'NNP'),\n",
       " (',', ','),\n",
       " ('along', 'IN'),\n",
       " ('with', 'IN'),\n",
       " ('Dale', 'NNP'),\n",
       " ('Dougherty', 'NNP'),\n",
       " (',', ','),\n",
       " ('creator', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Make', 'NNP'),\n",
       " (':', ':'),\n",
       " ('and', 'CC'),\n",
       " ('Makerfaire', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('Marc', 'NNP'),\n",
       " ('Greuther', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('chief', 'JJ'),\n",
       " ('curator', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('museum', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS Tagging\n",
    "pos_tagged_tokens = [nltk.pos_tag(t) for t in tokens]\n",
    "pos_tagged_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:43.124396Z",
     "start_time": "2018-02-19T21:05:43.120272Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chunking\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:43.474162Z",
     "start_time": "2018-02-19T21:05:43.146804Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extraction\n",
    "ne_chunks = [nltk.ne_chunk(ptt) for ptt in pos_tagged_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:43.816160Z",
     "start_time": "2018-02-19T21:05:43.810467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  This/DT\n",
      "  morning/NN\n",
      "  I/PRP\n",
      "  had/VBD\n",
      "  the/DT\n",
      "  chance/NN\n",
      "  to/TO\n",
      "  get/VB\n",
      "  a/DT\n",
      "  tour/NN\n",
      "  of/IN\n",
      "  The/DT\n",
      "  (PERSON Henry/NNP Ford/NNP Museum/NNP)\n",
      "  in/IN\n",
      "  (GPE Dearborn/NNP)\n",
      "  ,/,\n",
      "  (ORGANIZATION MI/NNP)\n",
      "  ,/,\n",
      "  along/IN\n",
      "  with/IN\n",
      "  (PERSON Dale/NNP Dougherty/NNP)\n",
      "  ,/,\n",
      "  creator/NN\n",
      "  of/IN\n",
      "  (GPE Make/NNP)\n",
      "  :/:\n",
      "  and/CC\n",
      "  (ORGANIZATION Makerfaire/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (PERSON Marc/NNP Greuther/NNP)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  chief/JJ\n",
      "  curator/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  museum/NN\n",
      "  ./.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunks[0].pprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:05:45.600869Z",
     "start_time": "2018-02-19T21:05:45.402234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/chrisgiler_developer/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/chrisgiler_developer/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:06:13.899083Z",
     "start_time": "2018-02-19T21:06:13.894991Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:24:24.511935Z",
     "start_time": "2018-02-19T21:24:24.303633Z"
    }
   },
   "outputs": [],
   "source": [
    "N = 100 # Number of words to consider\n",
    "CLUSTER_THRESHOLD = 5 # Distance between words to consider\n",
    "TOP_SENTENCES = 2 # Number of sentences to return for a \"top n\" summary\n",
    "\n",
    "def _score_sentences(sentences, important_words):\n",
    "    scores = []\n",
    "    sentence_idx = -1\n",
    "    \n",
    "    for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "        sentence_idx += 1\n",
    "        word_idx = []\n",
    "        \n",
    "        for w in important_words:\n",
    "            try:\n",
    "                word_idx.append(s.index(w))\n",
    "            except ValueError or e:\n",
    "                pass\n",
    "        word_idx.sort()\n",
    "\n",
    "        if len(word_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        clusters = []\n",
    "        cluster = [word_idx[0]]\n",
    "        i = 1\n",
    "        while i < len(word_idx):\n",
    "            if word_idx[1] - word_idx[i - 1] < CLUSTER_THRESHOLD:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster = [word_idx[i]]\n",
    "            i += 1\n",
    "        clusters.append(cluster)\n",
    "\n",
    "        max_cluster_score = 0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster = len(c)\n",
    "            total_words_in_cluster = c[-1] - c[0] + 1\n",
    "            score = 1.0 * significant_words_in_cluster * significant_words_in_cluster / total_words_in_cluster\n",
    "            if score > max_cluster_score:\n",
    "                max_cluster_score = score\n",
    "        scores.append((sentence_idx, score))\n",
    "    return scores\n",
    "\n",
    "def summarize(txt):\n",
    "    sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "    \n",
    "    words = [w.lower() for sentence in normalized_sentences for w in nltk.tokenize.word_tokenize(sentence)]\n",
    "    \n",
    "    fdist = nltk.FreqDist(words)\n",
    "    \n",
    "    top_n_words = [w[0] for w in fdist.items() if w[0] not in nltk.corpus.stopwords.words('english')][:N]\n",
    "    \n",
    "    scored_sentences = _score_sentences(normalized_sentences, top_n_words)\n",
    "    \n",
    "    avg = np.mean([s[1] for s in scored_sentences])\n",
    "    std = np.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences if score > avg + 0.5 * std]\n",
    "    \n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-TOP_SENTENCES:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "    \n",
    "    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],\n",
    "                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:34:27.127344Z",
     "start_time": "2018-02-19T21:34:27.121511Z"
    }
   },
   "outputs": [],
   "source": [
    "txt = \"\"\"Alex Irpan, a software engineer at Google, wrote an excellent article on the current difficulties of getting deep reinforcement learning to work. For example, even after weeks of optimizing hyperparameters and explotation-exploration rates, these models are still highly sensitive to initial conditions. A 30% failure rate is seen as “working.”\n",
    "\n",
    "Irpan makes the argument that most attempts with deep RL fail but no one talks about it publicly, we only see the few cases where the problems are simplified enough to be feasible. He’s optimistic though. This is still a new field - the breakthrough Atari DQN paper was published only 3 years ago - so there is plenty of room for more research and advancement.\n",
    "\n",
    "Deep Reinforcement Learning for Trading\n",
    "From Denny Britz, a nice write up on using deep reinforcement learning for financial trading. This isn’t a tutorial with code, but rather about the difficulties and where reinforcement learning can fit in to this problem. Even if you have very limited knowledge of trading (like me), this article is a great starting point in a really interesting area.\n",
    "\n",
    "Sky-High Wages for AI Experts\n",
    "This week, Bloomberg reported on the extreme demand for AI experts. According to a report by Element AI, only 22,000 people are capable of building AI systems worldwide. Tencent recently reported the number to be around 200,000, so let’s say it’s more like 100,000 worldwide. This limited supply of AI practitioners and the high demand for them have led to salaries reaching $300,000. This demand will likely continue for the next few years as more applications for deep learning are discovered and more organizations learn what AI can do for them.\n",
    "\n",
    "Simplifying High-Performance Computing\n",
    "Much of the success of deep learning has come from the gains of high-performance computing on GPUs. So far this has mostly been done with CUDA and CuDNN, a library for deep learning build on CUDA. TensorFlow, PyTorch, and other current frameworks all run on CuDNN for efficient training and inference. As an alternative, Facebook announced Tensor Comprehensions, a C++ library that simplifies compiling machine learning applications for high-performance on GPUs and CPUs. While this still takes some computer engineering expertise, it should lower the bar for anyone looking to optimize their code.\n",
    "\n",
    "\"\"\".replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:34:27.826406Z",
     "start_time": "2018-02-19T21:34:27.820536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alex Irpan, a software engineer at Google, wrote an excellent article on the current difficulties of getting deep reinforcement learning to work. For example, even after weeks of optimizing hyperparameters and explotation-exploration rates, these models are still highly sensitive to initial conditions. A 30% failure rate is seen as “working.”  Irpan makes the argument that most attempts with deep RL fail but no one talks about it publicly, we only see the few cases where the problems are simplified enough to be feasible. He’s optimistic though. This is still a new field - the breakthrough Atari DQN paper was published only 3 years ago - so there is plenty of room for more research and advancement.  Deep Reinforcement Learning for Trading From Denny Britz, a nice write up on using deep reinforcement learning for financial trading. This isn’t a tutorial with code, but rather about the difficulties and where reinforcement learning can fit in to this problem. Even if you have very limited knowledge of trading (like me), this article is a great starting point in a really interesting area.  Sky-High Wages for AI Experts This week, Bloomberg reported on the extreme demand for AI experts. According to a report by Element AI, only 22,000 people are capable of building AI systems worldwide. Tencent recently reported the number to be around 200,000, so let’s say it’s more like 100,000 worldwide. This limited supply of AI practitioners and the high demand for them have led to salaries reaching $300,000. This demand will likely continue for the next few years as more applications for deep learning are discovered and more organizations learn what AI can do for them.  Simplifying High-Performance Computing Much of the success of deep learning has come from the gains of high-performance computing on GPUs. So far this has mostly been done with CUDA and CuDNN, a library for deep learning build on CUDA. TensorFlow, PyTorch, and other current frameworks all run on CuDNN for efficient training and inference. As an alternative, Facebook announced Tensor Comprehensions, a C++ library that simplifies compiling machine learning applications for high-performance on GPUs and CPUs. While this still takes some computer engineering expertise, it should lower the bar for anyone looking to optimize their code.  '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T21:36:24.364036Z",
     "start_time": "2018-02-19T21:36:24.221381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_scored_summary': ['Alex Irpan, a software engineer at Google, wrote an excellent article on the current difficulties of getting deep reinforcement learning to work.',\n",
       "  'For example, even after weeks of optimizing hyperparameters and explotation-exploration rates, these models are still highly sensitive to initial conditions.',\n",
       "  'A 30% failure rate is seen as “working.”  Irpan makes the argument that most attempts with deep RL fail but no one talks about it publicly, we only see the few cases where the problems are simplified enough to be feasible.',\n",
       "  'This is still a new field - the breakthrough Atari DQN paper was published only 3 years ago - so there is plenty of room for more research and advancement.',\n",
       "  'Even if you have very limited knowledge of trading (like me), this article is a great starting point in a really interesting area.'],\n",
       " 'top_n_summary': ['Alex Irpan, a software engineer at Google, wrote an excellent article on the current difficulties of getting deep reinforcement learning to work.',\n",
       "  'A 30% failure rate is seen as “working.”  Irpan makes the argument that most attempts with deep RL fail but no one talks about it publicly, we only see the few cases where the problems are simplified enough to be feasible.']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_SENTENCES = 2\n",
    "summarize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
